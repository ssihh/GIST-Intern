{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측을 위해 사용하는 데이터를 훈련 데이터셋(training dataset)이라고 합니다.  \n",
    "학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋을 테스트 데이터셋(test dataset)이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 합니다.  \n",
    "그리고 입력과 출력을 각기 다른 텐서에 저장할 필요가 있습니다. 이때 보편적으로 입력은 x, 출력은 y를 사용하여 표기합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가설의 H를 따서 y 대신 다음과 같이 식을 표현하기도 합니다.  \n",
    "이때 x와 곱해지는 **W를 가중치(Weight)**라고 하며, b를 편향(bias)이라고 합니다.  \n",
    "**H(x)=Wx+b**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**비용 함수(cost function) = 손실 함수(loss function)** = 오차 함수(error function) = 목적 함수(objective function)  \n",
    "특히 비용 함수와 손실 함수란 용어는 기억해두는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 데이터의 개수인 n으로 나누면, 오차의 제곱합에 대한 평균을 구할 수 있는데 이를 **평균 제곱 오차(Mean Squared Error, MSE)**라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 앞서 정의한 비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾는 방법에 대해서 배울 차례입니다.  \n",
    "이때 사용되는 것이 **옵티마이저(Optimizer)** 알고리즘입니다. **최적화 알고리즘**이라고도 부릅니다.  \n",
    "그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부릅니다.  \n",
    "여기서는 가장 기본적인 옵티마이저 알고리즘인 **경사 하강법(Gradient Descent)**에 대해서 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 W를 찾는 일이므로, 맨 아래의 볼록한 부분의 W의 값을 찾아야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률(learning rate)이라고 말하는 α는 어떤 의미를 가질까요? **학습률 α은 W의 값을 변경할 때, 얼마나 크게 변경할지를 결정합니다.**  \n",
    "또는 W를 그래프의 한 점으로보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할지를 결정합니다.  \n",
    "직관적으로 생각하기에 학습률 α의 값을 무작정 크게 하면 접선의 기울기가 최소값이 되는 W를 빠르게 찾을 수 있을 것같지만 그렇지 않습니다."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "위의 그림은 학습률 α가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 W를 찾아가는 것이 아니라 W의 값이 발산하는 상황을 보여줍니다.   \n",
    "반대로 학습률 α가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 α의 값을 찾아내는 것도 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념입니다. 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있으며 **선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차**, **옵티마이저는 경사 하강법**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "## 4. 파이토치로 선형 회귀 구현하기\n",
    "### 1. 기본 셋팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. torch\n",
    "메인 네임스페이스입니다. 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조를 가집니다.  \n",
    "\n",
    "#### 3. torch.nn  \n",
    "신경망을 구축하기 위한 다양한 데이터 구조나 레이어 등이 정의되어져 있습니다.  \n",
    "예를 들어 RNN, LSTM과 같은 레이어, ReLU와 같은 활성화 함수, MSELoss와 같은 손실 함수들이 있습니다.  \n",
    "\n",
    "#### 4. torch.optim  \n",
    "확률적 경사 하강법(Stochastic Gradient Descent, SGD)를 중심으로 한 파라미터 최적화 알고리즘이 구현되어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bae9c79450>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터는 난수를 그냥 생성하는 것이 아닌 난수 생성 알고리즘을 사용해 난수를 생성한다고 합니다!!!(그래서 어려운가 봅니다)  \n",
    "이때 이 **난수 알고리즘 실행하기 위해 쓰는 수를 Seed(씨앗)**이라 부릅니다.  \n",
    "Seed를 11로 지정을 했을 때 값은 계속 계속!!!!!!! 같은 값만 나옵니다.  \n",
    "\n",
    "### 2. 변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 가중치와 편향의 초기화\n",
    "선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W와 b의 값을 찾는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "# 가중치 W를 출력\n",
    "print(W) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 편향 b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시\n",
    "b = torch.zeros(1, requires_grad=True) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 가설 세우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 비용 함수 선언하기\n",
    "파이토치 코드 상으로 **선형 회귀**의 **비용 함수**에 해당되는 **평균 제곱 오차를 선언**합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 torch.mean으로 평균을 구한다.\n",
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 경사 하강법 구현하기\n",
    "경사 하강법을 구현합니다. 아래의 'SGD'는 경사 하강법의 일종입니다.  \n",
    "lr은 학습률(learning rate)를 의미합니다. 학습 대상인 W와 b가 SGD의 입력이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "# W와 b를 업데이트\n",
    "optimizer.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화**합니다.  \n",
    "기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.  \n",
    "그 다음 **cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됩니다.**   \n",
    "그 다음 경사 하강법 최적화 함수 opimizer의 .step() 함수를 호출하여,  \n",
    "인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트합니다.\n",
    "### 7. 전체코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad() # 미분값이 누적되므로, 미분값을 계속 0으로 초기화\n",
    "    cost.backward() # 자동으로 미분이 계산\n",
    "    optimizer.step() # lr 계산.. 업데이트\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**에포크(Epoch)**는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 말합니다.  \n",
    "이번 실습의 경우 2,000번을 수행했습니다.  \n",
    "\n",
    "최종 훈련 결과를 보면 최적의 기울기 W는 2에 가깝고, b는 0에 가까운 것을 볼 수 있습니다.  \n",
    "현재 훈련 데이터가 x_train은 [[1], [2], [3]]이고 y_train은 [[2], [4], [6]]인 것을 감안하면  \n",
    "실제 정답은 W가 2이고, b가 0인 H(x)=2x이므로 거의 정답을 찾은 셈입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. optimizer.zero_grad()가 필요한 이유\n",
    "파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기 값에 누적시키는 특징이 있습니다. 예를 들어봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 2.0\n",
      "수식을 w로 미분한 값 : 4.0\n",
      "수식을 w로 미분한 값 : 6.0\n",
      "수식을 w로 미분한 값 : 8.0\n",
      "수식을 w로 미분한 값 : 10.0\n",
      "수식을 w로 미분한 값 : 12.0\n",
      "수식을 w로 미분한 값 : 14.0\n",
      "수식을 w로 미분한 값 : 16.0\n",
      "수식을 w로 미분한 값 : 18.0\n",
      "수식을 w로 미분한 값 : 20.0\n",
      "수식을 w로 미분한 값 : 22.0\n",
      "수식을 w로 미분한 값 : 24.0\n",
      "수식을 w로 미분한 값 : 26.0\n",
      "수식을 w로 미분한 값 : 28.0\n",
      "수식을 w로 미분한 값 : 30.0\n",
      "수식을 w로 미분한 값 : 32.0\n",
      "수식을 w로 미분한 값 : 34.0\n",
      "수식을 w로 미분한 값 : 36.0\n",
      "수식을 w로 미분한 값 : 38.0\n",
      "수식을 w로 미분한 값 : 40.0\n",
      "수식을 w로 미분한 값 : 42.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "  z = 2*w\n",
    "\n",
    "  z.backward()\n",
    "  print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계속해서 미분값인 2가 누적되는 것을 볼 수 있습니다.  \n",
    "그렇기 때문에 optimizer.zero_grad()를 통해 미분값을 계속 0으로 초기화시켜줘야 합니다.\n",
    "## 6. torch.manual_seed()를 하는 이유\n",
    "torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다.  \n",
    "torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다는 특징때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 시드가 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n",
      "랜덤 시드가 5일 때\n",
      "tensor([0.8303])\n",
      "tensor([0.1261])\n",
      "랜덤 시드가 다시 3일 때\n",
      "tensor([0.0043])\n",
      "tensor([0.1056])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))\n",
    "\n",
    "torch.manual_seed(5)\n",
    "print('랜덤 시드가 5일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))\n",
    "\n",
    "torch.manual_seed(3)\n",
    "print('랜덤 시드가 다시 3일 때')\n",
    "for i in range(1,3):\n",
    "  print(torch.rand(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서에는 **requires_grad**라는 속성이 있습니다. 이것을 True로 설정하면 자동 미분 기능이 적용됩니다.  \n",
    "선형 회귀부터 신경망과 같은 복잡한 구조에서 파라미터들이 모두 이 기능이 적용됩니다.  \n",
    "**requires_grad = True**가 적용된 텐서에 연산을 하면, **계산 그래프가 생성**되며   \n",
    "**backward** 함수를 호출하면 그래프로부터 **자동으로 미분이 계산**됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
